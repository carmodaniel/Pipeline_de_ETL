# ğŸš€ Pipeline de ETL com Python â€” Olist Brazilian E-Commerce

![Status](https://img.shields.io/badge/status-active-brightgreen)
![Python](https://img.shields.io/badge/python-3.10+-blue)
![License](https://img.shields.io/badge/license-MIT-lightgrey)
![Contributions](https://img.shields.io/badge/contributions-welcome-orange)
![SQLite](https://img.shields.io/badge/database-SQLite-blue)
![ETL](https://img.shields.io/badge/process-ETL-success)

Projeto completo de **Engenharia de Dados**, implementando um **Pipeline ETL** utilizando o dataset  
**Brazilian E-Commerce Public Dataset by Olist (Kaggle)**.

Este pipeline abrange:

- **Extract** â†’ Leitura dos arquivos CSV brutos  
- **Transform** â†’ Limpeza, enriquecimento e criaÃ§Ã£o de mÃ©tricas  
- **Load CSV** â†’ GeraÃ§Ã£o de tabelas processadas  
- **Load SQLite** â†’ CriaÃ§Ã£o do banco `olist.db` + tabela `orders_processed`

---




## ğŸ”„ Fluxograma do Pipeline ETL

```mermaid
flowchart TD
    A[ğŸ“¥ Extract
Carregar CSVs de orders, items e customers] --> B[ğŸ”§ Transform
Limpeza, datas, joins, mÃ©tricas]
    B --> C[ğŸ’¾ Load CSV
Gerar orders_processed.csv]
    C --> D[ğŸŸ¦ Load SQLite
Criar banco olist.db]
    D --> E[ğŸ“Š Output
Dados prontos para anÃ¡lise]
```

---

## ğŸ“ Estrutura do DiretÃ³rio

```
etl_olist/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ load_sql.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚
â”œâ”€â”€ config.py
â””â”€â”€ requirements.txt
```

---

## ğŸ§ª MÃ³dulos do Projeto

### ğŸ“¥ 1. Extract â€” `src/extract.py`

```python
import pandas as pd
import os

def load_csv(file_name: str, base_path="data/raw/") -> pd.DataFrame:
    full_path = os.path.join(base_path, file_name)
    print(f"ğŸ“¥ Extraindo {file_name}...")
    return pd.read_csv(full_path)
```

---

### ğŸ”§ 2. Transform â€” `src/transform.py`

```python
import pandas as pd

def transform_orders(orders, items, customers):
    print("ğŸ”§ Transformando dados...")

    date_cols = [
        "order_purchase_timestamp",
        "order_approved_at",
        "order_delivered_carrier_date",
        "order_delivered_customer_date",
        "order_estimated_delivery_date"
    ]

    for col in date_cols:
        orders[col] = pd.to_datetime(orders[col], errors="coerce")

    orders["delivery_time_days"] = (
        orders["order_delivered_customer_date"] - orders["order_purchase_timestamp"]
    ).dt.days

    orders["delivery_delay_days"] = (
        orders["order_delivered_customer_date"] - orders["order_estimated_delivery_date"]
    ).dt.days

    items_grouped = items.groupby("order_id").agg(
        total_items=("order_item_id", "count"),
        total_value=("price", "sum"),
        total_freight=("freight_value", "sum")
    ).reset_index()

    orders = orders.merge(items_grouped, on="order_id", how="left")
    orders = orders.merge(customers[['customer_id','customer_city','customer_state']], 
                          on="customer_id", how="left")

    return orders
```

---

### ğŸ’¾ 3. Load CSV â€” `src/load.py`

```python
import os

def save_processed(df, filename="orders_processed.csv", base_path="data/processed/"):
    os.makedirs(base_path, exist_ok=True)
    full_path = os.path.join(base_path, filename)

    print(f"ğŸ’¾ Salvando dataset em: {full_path}")
    df.to_csv(full_path, index=False)
```

---

### ğŸŸ¦ 4. Load SQLite â€” `src/load_sql.py`

```python
import os
import sqlite3

def load_to_sqlite(df, db_path="data/processed/olist.db", table_name="orders_processed"):
    os.makedirs(os.path.dirname(db_path), exist_ok=True)

    print(f"ğŸŸ¦ Conectando ao SQLite: {db_path}")
    conn = sqlite3.connect(db_path)

    print(f"ğŸ“¤ Inserindo dados na tabela '{table_name}'...")
    df.to_sql(table_name, conn, if_exists="replace", index=False)

    conn.close()
    print("âœ… InserÃ§Ã£o concluÃ­da!")
```

---

### â–¶ï¸ 5. Pipeline â€” `src/pipeline.py`

```python
from extract import load_csv
from transform import transform_orders
from load import save_processed
from load_sql import load_to_sqlite

def run_pipeline():
    print("
ğŸš€ Iniciando Pipeline ETL Olist...
")

    orders = load_csv("olist_orders_dataset.csv")
    items = load_csv("olist_order_items_dataset.csv")
    customers = load_csv("olist_customers_dataset.csv")

    df_final = transform_orders(orders, items, customers)

    save_processed(df_final)
    load_to_sqlite(df_final)

    print("
ğŸ‰ Pipeline ETL finalizado com sucesso!
")

if __name__ == "__main__":
    run_pipeline()
```

---

## â–¶ï¸ ExecuÃ§Ã£o

```bash
python src/pipeline.py
```

---

## ğŸ§ª Validar SQLite

```python
import sqlite3

conn = sqlite3.connect("data/processed/olist.db")
cursor = conn.cursor()

cursor.execute("SELECT * FROM orders_processed LIMIT 10")
print(cursor.fetchall())

conn.close()
```

---

## ğŸ“Š PrÃ³ximas EvoluÃ§Ãµes

- Criar modelo estrela (DimensÃµes e Fato)
- Implementar DAG no Airflow
- Desenvolver Data Quality Checks
- Criar dashboard Power BI/Looker
- Implementar carga incremental

---

## âœ¨ Autor

**Daniel Eduardo do Carmo**  
Engenharia de Dados â€¢ Python â€¢ ETL â€¢ SQLite â€¢ Kaggle

---

## ğŸ”— Links

- Dataset: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce  
- DB Browser: https://sqlitebrowser.org  
